{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pseudo-code for Neural Network Implementation\n",
        "\n",
        "1. **Initialize Neural Network Class**\n",
        "    - **Input Parameters:**\n",
        "        - `input_size`: Number of neurons in the input layer\n",
        "        - `hidden_size`: Number of neurons in the hidden layer\n",
        "        - `output_size`: Number of neurons in the output layer\n",
        "    \n",
        "    1. **Initialize Weights:**\n",
        "        - `weights_input_hidden` = initialize with random values, shape: (input_size, hidden_size)\n",
        "        - `weights_hidden_output` = initialize with random values, shape: (hidden_size, output_size)\n",
        "    \n",
        "    2. **Initialize Biases:**\n",
        "        - `bias_hidden` = initialize with zeros, shape: (1, hidden_size)\n",
        "        - `bias_output` = initialize with zeros, shape: (1, output_size)\n",
        "\n",
        "2. **Define Activation Function**\n",
        "    - **Sigmoid Function:**\n",
        "        - `sigmoid(x)` = 1 / (1 + exp(-x))\n",
        "\n",
        "3. **Define Activation Function Derivative**\n",
        "    - **Sigmoid Derivative Function:**\n",
        "        - `sigmoid_derivative(x)` = x * (1 - x)\n",
        "\n",
        "4. **Feedforward Process**\n",
        "    - **Input:**\n",
        "        - `X`: Input data\n",
        "    \n",
        "    1. **Compute Hidden Layer Activation:**\n",
        "        - `hidden_activation` = dot(X, weights_input_hidden) + bias_hidden\n",
        "    \n",
        "    2. **Apply Activation Function:**\n",
        "        - `hidden_output` = sigmoid(hidden_activation)\n",
        "    \n",
        "    3. **Compute Output Layer Activation:**\n",
        "        - `output_activation` = dot(hidden_output, weights_hidden_output) + bias_output\n",
        "    \n",
        "    4. **Apply Activation Function:**\n",
        "        - `predicted_output` = sigmoid(output_activation)\n",
        "    \n",
        "    5. **Return Output:**\n",
        "        - `predicted_output`\n",
        "\n",
        "5. **Backward Propagation**\n",
        "    - **Input:**\n",
        "        - `X`: Input data\n",
        "        - `y`: True labels\n",
        "        - `learning_rate`: Learning rate for weight updates\n",
        "    \n",
        "    1. **Compute Output Layer Error:**\n",
        "        - `output_error` = y - predicted_output\n",
        "    \n",
        "    2. **Compute Output Delta:**\n",
        "        - `output_delta` = output_error * sigmoid_derivative(predicted_output)\n",
        "    \n",
        "    3. **Compute Hidden Layer Error:**\n",
        "        - `hidden_error` = dot(output_delta, transpose(weights_hidden_output))\n",
        "    \n",
        "    4. **Compute Hidden Delta:**\n",
        "        - `hidden_delta` = hidden_error * sigmoid_derivative(hidden_output)\n",
        "    \n",
        "    5. **Update Weights and Biases:**\n",
        "        - `weights_hidden_output` += dot(transpose(hidden_output), output_delta) * learning_rate\n",
        "        - `bias_output` += sum(output_delta, axis=0) * learning_rate\n",
        "        - `weights_input_hidden` += dot(transpose(X), hidden_delta) * learning_rate\n",
        "        - `bias_hidden` += sum(hidden_delta, axis=0) * learning_rate\n",
        "\n",
        "6. **Training Process**\n",
        "    - **Input:**\n",
        "        - `X`: Input data\n",
        "        - `y`: True labels\n",
        "        - `epochs`: Number of training iterations\n",
        "        - `learning_rate`: Learning rate for training\n",
        "    \n",
        "    1. **For Each Epoch in Range(epochs):**\n",
        "        - Perform feedforward operation with `X`\n",
        "        - Perform backward propagation with `X`, `y`, and `learning_rate`\n",
        "        \n",
        "    2. **Optional:**\n",
        "        - Compute `loss` = mean squared error between `y` and `predicted_output`\n",
        "        - Print current `epoch` and `loss`\n",
        "\n",
        "7. **Test the Model**\n",
        "    - **Input:**\n",
        "        - `X`: Input data for testing\n",
        "    \n",
        "    1. **Perform Feedforward Operation:**\n",
        "        - `predicted_output` = feedforward(X)\n",
        "    \n",
        "    2. **Output:**\n",
        "        - Print predictions after training\n"
      ],
      "metadata": {
        "id": "fDYqYV85YM7t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI5jhJPzU8e5",
        "outputId": "5b37d747-f73c-4448-f728-ae07da50ec80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss:0.3123681363532573\n",
            "Epoch 4000, Loss:0.01325373569306696\n",
            "Epoch 8000, Loss:0.002913463200902989\n",
            "Predictions after training:\n",
            "[[0.04646747]\n",
            " [0.95245605]\n",
            " [0.95494126]\n",
            " [0.04117355]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        "\n",
        "        # Initialize the biases\n",
        "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.bias_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Input to hidden\n",
        "        self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = self.sigmoid(self.hidden_activation)\n",
        "\n",
        "        # Hidden to output\n",
        "        self.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.predicted_output = self.sigmoid(self.output_activation)\n",
        "\n",
        "        return self.predicted_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        # Compute the output layer error\n",
        "        output_error = y - self.predicted_output\n",
        "        output_delta = output_error * self.sigmoid_derivative(self.predicted_output)\n",
        "\n",
        "        # Compute the hidden layer error\n",
        "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.feedforward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "            if epoch % 4000 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f'Epoch {epoch}, Loss:{loss}')\n",
        "\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "# Test the trained model\n",
        "output = nn.feedforward(X)\n",
        "print(\"Predictions after training:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d6FcOh63Wk0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pseudo-code for Neural Network Training\n",
        "\n",
        "1. **Initialize weights and biases**\n",
        "    - weights = initialize_weights()\n",
        "    - biases = initialize_biases()\n",
        "\n",
        "2. **Define activation function and its derivative**\n",
        "    - activation_function = sigmoid()\n",
        "    - activation_derivative = sigmoid_derivative()\n",
        "\n",
        "3. **Define loss function and its derivative**\n",
        "    - loss_function = mean_squared_error()\n",
        "    - loss_derivative = mean_squared_error_derivative()\n",
        "\n",
        "4. **Train the model**\n",
        "    - For each epoch from 1 to num_epochs:\n",
        "        1. **Forward pass for each sample**\n",
        "            - For each sample `i` from 1 to num_samples:\n",
        "                1. **Input layer**:\n",
        "                    - input_layer = inputs[i]\n",
        "                \n",
        "                2. **Hidden layer**:\n",
        "                    - hidden_layer = activation_function(dot(weights[0], input_layer) + biases[0])\n",
        "                \n",
        "                3. **Output layer**:\n",
        "                    - output_layer = activation_function(dot(weights[1], hidden_layer) + biases[1])\n",
        "                \n",
        "                4. **Calculate error**:\n",
        "                    - error = loss_function(output_layer, targets[i])\n",
        "                \n",
        "        2. **Backward pass**\n",
        "            1. **Output layer**:\n",
        "                - output_error = loss_derivative(error)\n",
        "                - output_delta = output_error * activation_derivative(output_layer)\n",
        "            \n",
        "            2. **Hidden layer**:\n",
        "                - hidden_error = dot(weights[1].T, output_delta)\n",
        "                - hidden_delta = hidden_error * activation_derivative(hidden_layer)\n",
        "            \n",
        "            3. **Update weights**:\n",
        "                - weights[0] -= learning_rate * dot(hidden_delta, input_layer.T)\n",
        "                - weights[1] -= learning_rate * dot(output_delta, hidden_layer.T)\n",
        "            \n",
        "            4. **Update biases**:\n",
        "                - biases[0] -= learning_rate * hidden_delta\n",
        "                - biases[1] -= learning_rate * output_delta\n",
        "\n",
        "5. **Return trained model**\n",
        "    - return weights, biases\n"
      ],
      "metadata": {
        "id": "cvW1IE35W5Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.weights, self.biases = self.initialize_weights_and_biases()\n",
        "\n",
        "    def initialize_weights_and_biases(self):\n",
        "        weights = [\n",
        "            np.random.randn(self.hidden_size, self.input_size),\n",
        "            np.random.randn(self.output_size, self.hidden_size)\n",
        "        ]\n",
        "        biases = [\n",
        "            np.zeros((self.hidden_size, 1)),\n",
        "            np.zeros((self.output_size, 1))\n",
        "        ]\n",
        "        return weights, biases\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def mean_squared_error(self, predicted, target):\n",
        "        return np.mean(np.square(target - predicted))\n",
        "\n",
        "    def mean_squared_error_derivative(self, predicted, target):\n",
        "        return 2 * (predicted - target) / target.size\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Input layer\n",
        "        self.input_layer = X\n",
        "\n",
        "        # Hidden layer\n",
        "        self.hidden_layer = self.sigmoid(np.dot(self.weights[0], self.input_layer) + self.biases[0])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = self.sigmoid(np.dot(self.weights[1], self.hidden_layer) + self.biases[1])\n",
        "\n",
        "        return self.output_layer\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        # Output layer\n",
        "        output_error = self.mean_squared_error_derivative(self.output_layer, y)\n",
        "        output_delta = output_error * self.sigmoid_derivative(self.output_layer)\n",
        "\n",
        "        # Hidden layer\n",
        "        hidden_error = np.dot(self.weights[1].T, output_delta)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_layer)\n",
        "\n",
        "        # Weight updates\n",
        "        self.weights[1] -= learning_rate * np.dot(output_delta, self.hidden_layer.T)\n",
        "        self.weights[0] -= learning_rate * np.dot(hidden_delta, self.input_layer.T)\n",
        "\n",
        "        # Bias updates\n",
        "        self.biases[1] -= learning_rate * output_delta\n",
        "        self.biases[0] -= learning_rate * hidden_delta\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(X.shape[0]):\n",
        "                input_sample = X[i].reshape(-1, 1)\n",
        "                target = y[i].reshape(-1, 1)\n",
        "\n",
        "                # Forward pass\n",
        "                self.feedforward(input_sample)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(input_sample, target, learning_rate)\n",
        "\n",
        "            if epoch % 4000 == 0:\n",
        "                loss = self.mean_squared_error(self.feedforward(X.T), y.T)\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "# Example usage\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "# Test the trained model\n",
        "output = nn.feedforward(X.T)\n",
        "print(\"Predictions after training:\")\n",
        "print(output.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miTx65DoWlbQ",
        "outputId": "e35739d0-dfb4-48b1-bc80-ec74601bae2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.30883138346757016\n",
            "Epoch 4000, Loss: 0.0021081096521129374\n",
            "Epoch 8000, Loss: 0.0008837500303570066\n",
            "Predictions after training:\n",
            "[[0.01655621]\n",
            " [0.97250957]\n",
            " [0.97455531]\n",
            " [0.03228627]]\n"
          ]
        }
      ]
    }
  ]
}
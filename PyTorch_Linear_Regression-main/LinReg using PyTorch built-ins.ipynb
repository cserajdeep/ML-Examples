{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "elegant-inside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_optimizer in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: pytorch-ranger>=0.1.1 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from torch_optimizer) (0.1.1)\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from torch_optimizer) (1.5.1+cu101)\n",
      "Requirement already satisfied: future in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from torch>=1.1.0->torch_optimizer) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\csera\\appdata\\roaming\\python\\python36\\site-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
      "Requirement already satisfied: adabelief_pytorch==0.2.0 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from adabelief_pytorch==0.2.0) (1.5.1+cu101)\n",
      "Requirement already satisfied: tabulate>=0.7 in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from adabelief_pytorch==0.2.0) (0.8.7)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\csera\\appdata\\roaming\\python\\python36\\site-packages (from adabelief_pytorch==0.2.0) (0.4.4)\n",
      "Requirement already satisfied: future in c:\\users\\csera\\.conda\\envs\\torch-siamese\\lib\\site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\csera\\appdata\\roaming\\python\\python36\\site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_optimizer\n",
    "!pip install adabelief_pytorch==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southeast-artwork",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 75.,  63.,  48.],\n",
      "        [ 91.,  88.,  66.],\n",
      "        [ 84., 137.,  57.],\n",
      "        [108.,  41.,  36.],\n",
      "        [ 68.,  98.,  72.]])\n",
      "tensor([[ 54.,  71.],\n",
      "        [ 84., 111.],\n",
      "        [122., 143.],\n",
      "        [ 20.,  40.],\n",
      "        [101., 117.]])\n",
      "tensor([[108.,  41.,  36.],\n",
      "        [ 91.,  88.,  66.]])\n",
      "tensor([[ 20.,  40.],\n",
      "        [ 84., 111.]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2608,  0.1790, -0.1053],\n",
      "        [ 0.1324,  0.3835,  0.2911]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4452, -0.1998], requires_grad=True)\n",
      "Epoch [10/1000], Loss: 151.2701\n",
      "Epoch [20/1000], Loss: 1369.3306\n",
      "Epoch [30/1000], Loss: 159.7799\n",
      "Epoch [40/1000], Loss: 720.9719\n",
      "Epoch [50/1000], Loss: 708.2392\n",
      "Epoch [60/1000], Loss: 1132.3232\n",
      "Epoch [70/1000], Loss: 1010.6023\n",
      "Epoch [80/1000], Loss: 755.7871\n",
      "Epoch [90/1000], Loss: 633.9187\n",
      "Epoch [100/1000], Loss: 520.8732\n",
      "Epoch [110/1000], Loss: 448.6373\n",
      "Epoch [120/1000], Loss: 1425.9229\n",
      "Epoch [130/1000], Loss: 412.9175\n",
      "Epoch [140/1000], Loss: 372.8903\n",
      "Epoch [150/1000], Loss: 11.5169\n",
      "Epoch [160/1000], Loss: 291.6708\n",
      "Epoch [170/1000], Loss: 8.2011\n",
      "Epoch [180/1000], Loss: 262.9291\n",
      "Epoch [190/1000], Loss: 77.7085\n",
      "Epoch [200/1000], Loss: 79.4800\n",
      "Epoch [210/1000], Loss: 227.5045\n",
      "Epoch [220/1000], Loss: 1217.8510\n",
      "Epoch [230/1000], Loss: 1184.3936\n",
      "Epoch [240/1000], Loss: 12.9193\n",
      "Epoch [250/1000], Loss: 12.3772\n",
      "Epoch [260/1000], Loss: 178.8397\n",
      "Epoch [270/1000], Loss: 1014.5062\n",
      "Epoch [280/1000], Loss: 155.3374\n",
      "Epoch [290/1000], Loss: 955.2894\n",
      "Epoch [300/1000], Loss: 16.0996\n",
      "Epoch [310/1000], Loss: 876.3287\n",
      "Epoch [320/1000], Loss: 58.2514\n",
      "Epoch [330/1000], Loss: 55.1978\n",
      "Epoch [340/1000], Loss: 49.4394\n",
      "Epoch [350/1000], Loss: 15.6126\n",
      "Epoch [360/1000], Loss: 97.5299\n",
      "Epoch [370/1000], Loss: 123.5508\n",
      "Epoch [380/1000], Loss: 79.1042\n",
      "Epoch [390/1000], Loss: 109.1517\n",
      "Epoch [400/1000], Loss: 67.9154\n",
      "Epoch [410/1000], Loss: 53.9066\n",
      "Epoch [420/1000], Loss: 575.7302\n",
      "Epoch [430/1000], Loss: 39.9958\n",
      "Epoch [440/1000], Loss: 39.6252\n",
      "Epoch [450/1000], Loss: 37.2935\n",
      "Epoch [460/1000], Loss: 453.3356\n",
      "Epoch [470/1000], Loss: 36.2171\n",
      "Epoch [480/1000], Loss: 76.5057\n",
      "Epoch [490/1000], Loss: 30.1629\n",
      "Epoch [500/1000], Loss: 27.1644\n",
      "Epoch [510/1000], Loss: 340.6704\n",
      "Epoch [520/1000], Loss: 21.7375\n",
      "Epoch [530/1000], Loss: 20.4988\n",
      "Epoch [540/1000], Loss: 20.5108\n",
      "Epoch [550/1000], Loss: 56.6003\n",
      "Epoch [560/1000], Loss: 56.6422\n",
      "Epoch [570/1000], Loss: 27.1345\n",
      "Epoch [580/1000], Loss: 14.6020\n",
      "Epoch [590/1000], Loss: 12.0528\n",
      "Epoch [600/1000], Loss: 10.2571\n",
      "Epoch [610/1000], Loss: 15.2229\n",
      "Epoch [620/1000], Loss: 14.7354\n",
      "Epoch [630/1000], Loss: 25.3701\n",
      "Epoch [640/1000], Loss: 167.3349\n",
      "Epoch [650/1000], Loss: 13.9953\n",
      "Epoch [660/1000], Loss: 132.2096\n",
      "Epoch [670/1000], Loss: 30.2266\n",
      "Epoch [680/1000], Loss: 30.4156\n",
      "Epoch [690/1000], Loss: 30.4452\n",
      "Epoch [700/1000], Loss: 27.5004\n",
      "Epoch [710/1000], Loss: 34.4337\n",
      "Epoch [720/1000], Loss: 82.1484\n",
      "Epoch [730/1000], Loss: 26.1260\n",
      "Epoch [740/1000], Loss: 27.1725\n",
      "Epoch [750/1000], Loss: 6.3033\n",
      "Epoch [760/1000], Loss: 0.6289\n",
      "Epoch [770/1000], Loss: 7.8493\n",
      "Epoch [780/1000], Loss: 0.6335\n",
      "Epoch [790/1000], Loss: 6.1395\n",
      "Epoch [800/1000], Loss: 5.5492\n",
      "Epoch [810/1000], Loss: 0.4827\n",
      "Epoch [820/1000], Loss: 28.0697\n",
      "Epoch [830/1000], Loss: 0.9702\n",
      "Epoch [840/1000], Loss: 5.1698\n",
      "Epoch [850/1000], Loss: 15.4752\n",
      "Epoch [860/1000], Loss: 28.7099\n",
      "Epoch [870/1000], Loss: 18.4285\n",
      "Epoch [880/1000], Loss: 28.9188\n",
      "Epoch [890/1000], Loss: 13.7030\n",
      "Epoch [900/1000], Loss: 28.5292\n",
      "Epoch [910/1000], Loss: 12.4581\n",
      "Epoch [920/1000], Loss: 10.0919\n",
      "Epoch [930/1000], Loss: 9.7805\n",
      "Epoch [940/1000], Loss: 3.6114\n",
      "Epoch [950/1000], Loss: 26.9901\n",
      "Epoch [960/1000], Loss: 5.2939\n",
      "Epoch [970/1000], Loss: 4.3141\n",
      "Epoch [980/1000], Loss: 28.2401\n",
      "Epoch [990/1000], Loss: 4.6734\n",
      "Epoch [1000/1000], Loss: 7.6332\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "\n",
    "# Input (x, y, z)\n",
    "inputs = np.array([[75, 63, 48], \n",
    "                   [91, 88, 66], \n",
    "                   [84, 137, 57], \n",
    "                   [108, 41, 36], \n",
    "                   [68, 98, 72]], dtype='float32')\n",
    "\n",
    "# Targets (a, b)\n",
    "targets = np.array([[54, 71], \n",
    "                    [84, 111], \n",
    "                    [122, 143], \n",
    "                    [20, 40], \n",
    "                    [101, 117]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 2  # should be << number of observations\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "#Adam Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr = 1e-3, betas= (0.9, 0.99))\n",
    "\n",
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "great-failing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 55.7432,  73.5533],\n",
       "        [ 81.0681, 104.2623],\n",
       "        [124.9522, 144.0197],\n",
       "        [ 22.3101,  42.0516],\n",
       "        [ 98.5910, 120.0431]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "atlantic-nickel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[56.2263, 72.2677]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[72, 65., 42.]]))  #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "devoted-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 75.,  63.,  48.],\n",
      "        [ 91.,  88.,  66.],\n",
      "        [ 84., 137.,  57.],\n",
      "        [108.,  41.,  36.],\n",
      "        [ 68.,  98.,  72.]])\n",
      "tensor([[ 54.,  71.],\n",
      "        [ 84., 111.],\n",
      "        [122., 143.],\n",
      "        [ 20.,  40.],\n",
      "        [101., 117.]])\n",
      "tensor([[108.,  41.,  36.],\n",
      "        [ 75.,  63.,  48.]])\n",
      "tensor([[20., 40.],\n",
      "        [54., 71.]])\n",
      "Parameter containing:\n",
      "tensor([[-0.1411,  0.2815, -0.0965],\n",
      "        [ 0.0735, -0.2701,  0.2017]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1614,  0.0841], requires_grad=True)\n",
      "Epoch [10/1000], Loss: 8099.4263\n",
      "Epoch [20/1000], Loss: 417.1291\n",
      "Epoch [30/1000], Loss: 12522.0059\n",
      "Epoch [40/1000], Loss: 6698.9771\n",
      "Epoch [50/1000], Loss: 1407.1208\n",
      "Epoch [60/1000], Loss: 1096.3097\n",
      "Epoch [70/1000], Loss: 4128.1685\n",
      "Epoch [80/1000], Loss: 2240.5576\n",
      "Epoch [90/1000], Loss: 411.6175\n",
      "Epoch [100/1000], Loss: 4402.8174\n",
      "Epoch [110/1000], Loss: 448.1182\n",
      "Epoch [120/1000], Loss: 1499.8447\n",
      "Epoch [130/1000], Loss: 573.7936\n",
      "Epoch [140/1000], Loss: 945.6118\n",
      "Epoch [150/1000], Loss: 744.0758\n",
      "Epoch [160/1000], Loss: 1049.1144\n",
      "Epoch [170/1000], Loss: 126.5586\n",
      "Epoch [180/1000], Loss: 36.4630\n",
      "Epoch [190/1000], Loss: 43.1926\n",
      "Epoch [200/1000], Loss: 62.3696\n",
      "Epoch [210/1000], Loss: 77.1094\n",
      "Epoch [220/1000], Loss: 85.1975\n",
      "Epoch [230/1000], Loss: 3.3530\n",
      "Epoch [240/1000], Loss: 581.3594\n",
      "Epoch [250/1000], Loss: 527.4832\n",
      "Epoch [260/1000], Loss: 1313.6510\n",
      "Epoch [270/1000], Loss: 0.6031\n",
      "Epoch [280/1000], Loss: 104.5446\n",
      "Epoch [290/1000], Loss: 0.9784\n",
      "Epoch [300/1000], Loss: 104.7665\n",
      "Epoch [310/1000], Loss: 99.6350\n",
      "Epoch [320/1000], Loss: 390.9123\n",
      "Epoch [330/1000], Loss: 369.7348\n",
      "Epoch [340/1000], Loss: 66.8794\n",
      "Epoch [350/1000], Loss: 59.2854\n",
      "Epoch [360/1000], Loss: 95.7738\n",
      "Epoch [370/1000], Loss: 65.9836\n",
      "Epoch [380/1000], Loss: 2.9781\n",
      "Epoch [390/1000], Loss: 2.6755\n",
      "Epoch [400/1000], Loss: 2.2748\n",
      "Epoch [410/1000], Loss: 2.9102\n",
      "Epoch [420/1000], Loss: 69.8124\n",
      "Epoch [430/1000], Loss: 72.0445\n",
      "Epoch [440/1000], Loss: 70.9108\n",
      "Epoch [450/1000], Loss: 4.4623\n",
      "Epoch [460/1000], Loss: 667.4209\n",
      "Epoch [470/1000], Loss: 3.8091\n",
      "Epoch [480/1000], Loss: 3.8398\n",
      "Epoch [490/1000], Loss: 241.7791\n",
      "Epoch [500/1000], Loss: 60.3364\n",
      "Epoch [510/1000], Loss: 556.1470\n",
      "Epoch [520/1000], Loss: 215.2177\n",
      "Epoch [530/1000], Loss: 57.0800\n",
      "Epoch [540/1000], Loss: 199.6663\n",
      "Epoch [550/1000], Loss: 483.9322\n",
      "Epoch [560/1000], Loss: 55.8708\n",
      "Epoch [570/1000], Loss: 50.1183\n",
      "Epoch [580/1000], Loss: 17.6198\n",
      "Epoch [590/1000], Loss: 12.9767\n",
      "Epoch [600/1000], Loss: 158.5261\n",
      "Epoch [610/1000], Loss: 154.5240\n",
      "Epoch [620/1000], Loss: 356.7444\n",
      "Epoch [630/1000], Loss: 40.9901\n",
      "Epoch [640/1000], Loss: 325.7599\n",
      "Epoch [650/1000], Loss: 40.1751\n",
      "Epoch [660/1000], Loss: 9.7715\n",
      "Epoch [670/1000], Loss: 9.8656\n",
      "Epoch [680/1000], Loss: 33.5309\n",
      "Epoch [690/1000], Loss: 254.5532\n",
      "Epoch [700/1000], Loss: 11.3642\n",
      "Epoch [710/1000], Loss: 106.7160\n",
      "Epoch [720/1000], Loss: 218.5558\n",
      "Epoch [730/1000], Loss: 99.9105\n",
      "Epoch [740/1000], Loss: 209.3102\n",
      "Epoch [750/1000], Loss: 30.1545\n",
      "Epoch [760/1000], Loss: 78.9234\n",
      "Epoch [770/1000], Loss: 28.1361\n",
      "Epoch [780/1000], Loss: 0.7461\n",
      "Epoch [790/1000], Loss: 11.5449\n",
      "Epoch [800/1000], Loss: 25.0154\n",
      "Epoch [810/1000], Loss: 0.8611\n",
      "Epoch [820/1000], Loss: 63.9945\n",
      "Epoch [830/1000], Loss: 12.3840\n",
      "Epoch [840/1000], Loss: 13.6565\n",
      "Epoch [850/1000], Loss: 21.9615\n",
      "Epoch [860/1000], Loss: 19.4215\n",
      "Epoch [870/1000], Loss: 0.8280\n",
      "Epoch [880/1000], Loss: 0.8519\n",
      "Epoch [890/1000], Loss: 18.3436\n",
      "Epoch [900/1000], Loss: 12.6495\n",
      "Epoch [910/1000], Loss: 1.2414\n",
      "Epoch [920/1000], Loss: 15.5075\n",
      "Epoch [930/1000], Loss: 15.9634\n",
      "Epoch [940/1000], Loss: 16.4229\n",
      "Epoch [950/1000], Loss: 15.8785\n",
      "Epoch [960/1000], Loss: 25.1660\n",
      "Epoch [970/1000], Loss: 24.3921\n",
      "Epoch [980/1000], Loss: 11.8054\n",
      "Epoch [990/1000], Loss: 15.2110\n",
      "Epoch [1000/1000], Loss: 4.0128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "\n",
    "# Input (x, y, z)\n",
    "inputs = np.array([[75, 63, 48], \n",
    "                   [91, 88, 66], \n",
    "                   [84, 137, 57], \n",
    "                   [108, 41, 36], \n",
    "                   [68, 98, 72]], dtype='float32')\n",
    "\n",
    "# Targets (a, b)\n",
    "targets = np.array([[54, 71], \n",
    "                    [84, 111], \n",
    "                    [122, 143], \n",
    "                    [20, 40], \n",
    "                    [101, 117]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 2  # should be << number of observations\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "# RMSprop Optimizer\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr = 1e-3, alpha = 0.9)\n",
    "\n",
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "racial-premium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 55.9039,  75.3351],\n",
       "        [ 81.9113, 105.6856],\n",
       "        [122.8778, 137.4829],\n",
       "        [ 20.2848,  47.1356],\n",
       "        [100.8588, 119.5984]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unique-cedar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[55.6298, 72.9180]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[72, 65., 42.]]))  #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "elder-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 75.,  63.,  48.],\n",
      "        [ 91.,  88.,  66.],\n",
      "        [ 84., 137.,  57.],\n",
      "        [108.,  41.,  36.],\n",
      "        [ 68.,  98.,  72.]])\n",
      "tensor([[ 54.,  71.],\n",
      "        [ 84., 111.],\n",
      "        [122., 143.],\n",
      "        [ 20.,  40.],\n",
      "        [101., 117.]])\n",
      "tensor([[68., 98., 72.],\n",
      "        [75., 63., 48.]])\n",
      "tensor([[101., 117.],\n",
      "        [ 54.,  71.]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3741,  0.1702, -0.1889],\n",
      "        [-0.1988,  0.5024, -0.3746]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0806, -0.5676], requires_grad=True)\n",
      "Epoch [10/1000], Loss: 68.5174\n",
      "Epoch [20/1000], Loss: 85.3458\n",
      "Epoch [30/1000], Loss: 95.6487\n",
      "Epoch [40/1000], Loss: 124.4186\n",
      "Epoch [50/1000], Loss: 67.7092\n",
      "Epoch [60/1000], Loss: 53.6633\n",
      "Epoch [70/1000], Loss: 73.4481\n",
      "Epoch [80/1000], Loss: 33.6344\n",
      "Epoch [90/1000], Loss: 63.5969\n",
      "Epoch [100/1000], Loss: 38.8214\n",
      "Epoch [110/1000], Loss: 32.3306\n",
      "Epoch [120/1000], Loss: 23.1528\n",
      "Epoch [130/1000], Loss: 0.7507\n",
      "Epoch [140/1000], Loss: 14.0357\n",
      "Epoch [150/1000], Loss: 0.9158\n",
      "Epoch [160/1000], Loss: 17.3451\n",
      "Epoch [170/1000], Loss: 17.7729\n",
      "Epoch [180/1000], Loss: 1.2282\n",
      "Epoch [190/1000], Loss: 10.6428\n",
      "Epoch [200/1000], Loss: 11.6512\n",
      "Epoch [210/1000], Loss: 57.5356\n",
      "Epoch [220/1000], Loss: 61.7134\n",
      "Epoch [230/1000], Loss: 4.8337\n",
      "Epoch [240/1000], Loss: 25.3036\n",
      "Epoch [250/1000], Loss: 1.3584\n",
      "Epoch [260/1000], Loss: 9.5332\n",
      "Epoch [270/1000], Loss: 53.6663\n",
      "Epoch [280/1000], Loss: 5.1283\n",
      "Epoch [290/1000], Loss: 4.5470\n",
      "Epoch [300/1000], Loss: 16.2346\n",
      "Epoch [310/1000], Loss: 3.4306\n",
      "Epoch [320/1000], Loss: 2.3286\n",
      "Epoch [330/1000], Loss: 45.4457\n",
      "Epoch [340/1000], Loss: 41.0601\n",
      "Epoch [350/1000], Loss: 40.2161\n",
      "Epoch [360/1000], Loss: 13.6449\n",
      "Epoch [370/1000], Loss: 18.5419\n",
      "Epoch [380/1000], Loss: 2.9683\n",
      "Epoch [390/1000], Loss: 36.2477\n",
      "Epoch [400/1000], Loss: 2.3211\n",
      "Epoch [410/1000], Loss: 5.1152\n",
      "Epoch [420/1000], Loss: 5.3794\n",
      "Epoch [430/1000], Loss: 6.5875\n",
      "Epoch [440/1000], Loss: 44.0294\n",
      "Epoch [450/1000], Loss: 3.0513\n",
      "Epoch [460/1000], Loss: 2.0871\n",
      "Epoch [470/1000], Loss: 32.8894\n",
      "Epoch [480/1000], Loss: 2.7886\n",
      "Epoch [490/1000], Loss: 5.7016\n",
      "Epoch [500/1000], Loss: 31.5101\n",
      "Epoch [510/1000], Loss: 39.6557\n",
      "Epoch [520/1000], Loss: 39.0597\n",
      "Epoch [530/1000], Loss: 5.8673\n",
      "Epoch [540/1000], Loss: 2.2265\n",
      "Epoch [550/1000], Loss: 37.5975\n",
      "Epoch [560/1000], Loss: 2.3751\n",
      "Epoch [570/1000], Loss: 1.6833\n",
      "Epoch [580/1000], Loss: 37.1767\n",
      "Epoch [590/1000], Loss: 2.4137\n",
      "Epoch [600/1000], Loss: 36.3562\n",
      "Epoch [610/1000], Loss: 5.5679\n",
      "Epoch [620/1000], Loss: 29.8534\n",
      "Epoch [630/1000], Loss: 3.6083\n",
      "Epoch [640/1000], Loss: 2.2223\n",
      "Epoch [650/1000], Loss: 2.1841\n",
      "Epoch [660/1000], Loss: 35.3178\n",
      "Epoch [670/1000], Loss: 35.6084\n",
      "Epoch [680/1000], Loss: 31.4101\n",
      "Epoch [690/1000], Loss: 4.3118\n",
      "Epoch [700/1000], Loss: 1.6110\n",
      "Epoch [710/1000], Loss: 32.2862\n",
      "Epoch [720/1000], Loss: 4.8724\n",
      "Epoch [730/1000], Loss: 34.5407\n",
      "Epoch [740/1000], Loss: 0.3145\n",
      "Epoch [750/1000], Loss: 0.9059\n",
      "Epoch [760/1000], Loss: 26.6337\n",
      "Epoch [770/1000], Loss: 1.7910\n",
      "Epoch [780/1000], Loss: 4.2123\n",
      "Epoch [790/1000], Loss: 0.1676\n",
      "Epoch [800/1000], Loss: 5.3044\n",
      "Epoch [810/1000], Loss: 2.8658\n",
      "Epoch [820/1000], Loss: 0.1779\n",
      "Epoch [830/1000], Loss: 5.4090\n",
      "Epoch [840/1000], Loss: 6.6238\n",
      "Epoch [850/1000], Loss: 1.1642\n",
      "Epoch [860/1000], Loss: 0.1044\n",
      "Epoch [870/1000], Loss: 5.1349\n",
      "Epoch [880/1000], Loss: 1.6109\n",
      "Epoch [890/1000], Loss: 4.6834\n",
      "Epoch [900/1000], Loss: 5.5349\n",
      "Epoch [910/1000], Loss: 7.1844\n",
      "Epoch [920/1000], Loss: 0.2402\n",
      "Epoch [930/1000], Loss: 5.7902\n",
      "Epoch [940/1000], Loss: 0.1552\n",
      "Epoch [950/1000], Loss: 0.1301\n",
      "Epoch [960/1000], Loss: 4.6496\n",
      "Epoch [970/1000], Loss: 0.0038\n",
      "Epoch [980/1000], Loss: 5.0916\n",
      "Epoch [990/1000], Loss: 5.7776\n",
      "Epoch [1000/1000], Loss: 0.1037\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "\n",
    "# Input (x, y, z)\n",
    "inputs = np.array([[75, 63, 48], \n",
    "                   [91, 88, 66], \n",
    "                   [84, 137, 57], \n",
    "                   [108, 41, 36], \n",
    "                   [68, 98, 72]], dtype='float32')\n",
    "\n",
    "# Targets (a, b)\n",
    "targets = np.array([[54, 71], \n",
    "                    [84, 111], \n",
    "                    [122, 143], \n",
    "                    [20, 40], \n",
    "                    [101, 117]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 2  # should be << number of observations\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "# SGD Optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cubic-growth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 55.9152,  73.4186],\n",
       "        [ 81.9363, 104.2038],\n",
       "        [122.1121, 143.3028],\n",
       "        [ 19.9337,  41.6064],\n",
       "        [101.1401, 120.1965]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "known-electric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[55.4840, 71.9879]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[72, 65., 42.]]))  #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "driven-demand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 75.,  63.,  48.],\n",
      "        [ 91.,  88.,  66.],\n",
      "        [ 84., 137.,  57.],\n",
      "        [108.,  41.,  36.],\n",
      "        [ 68.,  98.,  72.]])\n",
      "tensor([[ 54.,  71.],\n",
      "        [ 84., 111.],\n",
      "        [122., 143.],\n",
      "        [ 20.,  40.],\n",
      "        [101., 117.]])\n",
      "tensor([[108.,  41.,  36.],\n",
      "        [ 91.,  88.,  66.]])\n",
      "tensor([[ 20.,  40.],\n",
      "        [ 84., 111.]])\n",
      "Parameter containing:\n",
      "tensor([[-0.0642,  0.2038,  0.3296],\n",
      "        [ 0.3397, -0.4588,  0.1628]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2240, -0.0778], requires_grad=True)\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Epoch [10/1000], Loss: 1599.5325\n",
      "Epoch [20/1000], Loss: 1026.4380\n",
      "Epoch [30/1000], Loss: 1597.9998\n",
      "Epoch [40/1000], Loss: 3481.8701\n",
      "Epoch [50/1000], Loss: 1028.2096\n",
      "Epoch [60/1000], Loss: 591.2183\n",
      "Epoch [70/1000], Loss: 590.3581\n",
      "Epoch [80/1000], Loss: 589.4880\n",
      "Epoch [90/1000], Loss: 1589.3118\n",
      "Epoch [100/1000], Loss: 31.5823\n",
      "Epoch [110/1000], Loss: 1033.9594\n",
      "Epoch [120/1000], Loss: 584.5468\n",
      "Epoch [130/1000], Loss: 3453.5581\n",
      "Epoch [140/1000], Loss: 30.7021\n",
      "Epoch [150/1000], Loss: 580.4224\n",
      "Epoch [160/1000], Loss: 1573.3044\n",
      "Epoch [170/1000], Loss: 29.9854\n",
      "Epoch [180/1000], Loss: 575.8658\n",
      "Epoch [190/1000], Loss: 1044.8239\n",
      "Epoch [200/1000], Loss: 572.6001\n",
      "Epoch [210/1000], Loss: 1047.8301\n",
      "Epoch [220/1000], Loss: 3413.2339\n",
      "Epoch [230/1000], Loss: 3407.9126\n",
      "Epoch [240/1000], Loss: 28.1115\n",
      "Epoch [250/1000], Loss: 1547.3939\n",
      "Epoch [260/1000], Loss: 3391.4438\n",
      "Epoch [270/1000], Loss: 27.2300\n",
      "Epoch [280/1000], Loss: 3380.2761\n",
      "Epoch [290/1000], Loss: 26.6662\n",
      "Epoch [300/1000], Loss: 3369.8599\n",
      "Epoch [310/1000], Loss: 1528.0148\n",
      "Epoch [320/1000], Loss: 550.6012\n",
      "Epoch [330/1000], Loss: 1521.3440\n",
      "Epoch [340/1000], Loss: 3347.2864\n",
      "Epoch [350/1000], Loss: 544.8497\n",
      "Epoch [360/1000], Loss: 3334.5637\n",
      "Epoch [370/1000], Loss: 1507.1035\n",
      "Epoch [380/1000], Loss: 1079.3765\n",
      "Epoch [390/1000], Loss: 1081.4441\n",
      "Epoch [400/1000], Loss: 534.4283\n",
      "Epoch [410/1000], Loss: 1493.1840\n",
      "Epoch [420/1000], Loss: 530.4824\n",
      "Epoch [430/1000], Loss: 528.4821\n",
      "Epoch [440/1000], Loss: 22.2787\n",
      "Epoch [450/1000], Loss: 1093.4694\n",
      "Epoch [460/1000], Loss: 21.6725\n",
      "Epoch [470/1000], Loss: 1471.2216\n",
      "Epoch [480/1000], Loss: 517.9466\n",
      "Epoch [490/1000], Loss: 20.8451\n",
      "Epoch [500/1000], Loss: 3248.6191\n",
      "Epoch [510/1000], Loss: 1105.8903\n",
      "Epoch [520/1000], Loss: 509.5524\n",
      "Epoch [530/1000], Loss: 19.7144\n",
      "Epoch [540/1000], Loss: 19.4377\n",
      "Epoch [550/1000], Loss: 503.1894\n",
      "Epoch [560/1000], Loss: 3209.8196\n",
      "Epoch [570/1000], Loss: 18.5991\n",
      "Epoch [580/1000], Loss: 496.7259\n",
      "Epoch [590/1000], Loss: 18.0470\n",
      "Epoch [600/1000], Loss: 1422.9429\n",
      "Epoch [610/1000], Loss: 17.5169\n",
      "Epoch [620/1000], Loss: 1415.0493\n",
      "Epoch [630/1000], Loss: 16.9898\n",
      "Epoch [640/1000], Loss: 16.7539\n",
      "Epoch [650/1000], Loss: 1137.1969\n",
      "Epoch [660/1000], Loss: 1400.3120\n",
      "Epoch [670/1000], Loss: 477.5678\n",
      "Epoch [680/1000], Loss: 3130.1782\n",
      "Epoch [690/1000], Loss: 15.4684\n",
      "Epoch [700/1000], Loss: 3116.5820\n",
      "Epoch [710/1000], Loss: 1151.7413\n",
      "Epoch [720/1000], Loss: 14.7206\n",
      "Epoch [730/1000], Loss: 14.4853\n",
      "Epoch [740/1000], Loss: 1368.9813\n",
      "Epoch [750/1000], Loss: 1160.5109\n",
      "Epoch [760/1000], Loss: 13.8558\n",
      "Epoch [770/1000], Loss: 3071.3267\n",
      "Epoch [780/1000], Loss: 1167.2567\n",
      "Epoch [790/1000], Loss: 3058.0227\n",
      "Epoch [800/1000], Loss: 1171.9468\n",
      "Epoch [810/1000], Loss: 3044.5649\n",
      "Epoch [820/1000], Loss: 1176.5421\n",
      "Epoch [830/1000], Loss: 3031.4084\n",
      "Epoch [840/1000], Loss: 441.5576\n",
      "Epoch [850/1000], Loss: 11.9451\n",
      "Epoch [860/1000], Loss: 1185.8479\n",
      "Epoch [870/1000], Loss: 3004.6560\n",
      "Epoch [880/1000], Loss: 433.3434\n",
      "Epoch [890/1000], Loss: 431.2278\n",
      "Epoch [900/1000], Loss: 429.1380\n",
      "Epoch [910/1000], Loss: 1197.5701\n",
      "Epoch [920/1000], Loss: 10.6282\n",
      "Epoch [930/1000], Loss: 10.4432\n",
      "Epoch [940/1000], Loss: 2959.3223\n",
      "Epoch [950/1000], Loss: 1207.0848\n",
      "Epoch [960/1000], Loss: 417.0990\n",
      "Epoch [970/1000], Loss: 9.7723\n",
      "Epoch [980/1000], Loss: 2933.7820\n",
      "Epoch [990/1000], Loss: 9.4578\n",
      "Epoch [1000/1000], Loss: 1218.6630\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer as optim\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "\n",
    "# Input (x, y, z)\n",
    "inputs = np.array([[75, 63, 48], \n",
    "                   [91, 88, 66], \n",
    "                   [84, 137, 57], \n",
    "                   [108, 41, 36], \n",
    "                   [68, 98, 72]], dtype='float32')\n",
    "\n",
    "# Targets (a, b)\n",
    "targets = np.array([[54, 71], \n",
    "                    [84, 111], \n",
    "                    [122, 143], \n",
    "                    [20, 40], \n",
    "                    [101, 117]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 2  # should be << number of observations\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n",
    "    \n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "#Adabelief Optimizer\n",
    "#opt = optim.AdaBelief(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8)\n",
    "opt = AdaBelief(model.parameters(), lr=1e-5, eps=1e-16, betas=(0.9, 0.999), print_change_log = False)\n",
    "\n",
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opponent-machine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[54.7667, 66.7601],\n",
       "        [74.7739, 83.9230],\n",
       "        [90.1538, 73.5396],\n",
       "        [42.8573, 83.7607],\n",
       "        [79.7780, 71.2603]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "running-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52.5423, 62.1362]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[72, 65., 42.]]))  #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-clearance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
